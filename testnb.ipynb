{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-29T13:01:19.413331Z",
     "start_time": "2024-10-29T13:01:15.444972Z"
    }
   },
   "source": "%pip install llamaapi",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llamaapi in c:\\users\\anwender\\anaconda3\\lib\\site-packages (0.1.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.5 in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from llamaapi) (3.8.5)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.6 in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from llamaapi) (1.5.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.27.1 in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from llamaapi) (2.31.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (1.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.27.1->llamaapi) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.27.1->llamaapi) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.27.1->llamaapi) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T13:01:24.318408Z",
     "start_time": "2024-10-29T13:01:22.540409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from llamaapi import LlamaAPI\n",
    "\n",
    "# Initialize the SDK\n",
    "llama = LlamaAPI(\"api_key\")\n",
    "\n",
    "# Build the API request\n",
    "api_request_json = {\n",
    "    \"model\": \"llama3.1-70b\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is the weather like in Vienna?\"},\n",
    "    ],\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"days\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"for how many days ahead you wants the forecast\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"location\", \"days\"],\n",
    "        }\n",
    "    ],\n",
    "    \"stream\": False,\n",
    "    \"function_call\": \"get_current_weather\",\n",
    "}\n",
    "\n",
    "# Execute the Request\n",
    "response = llama.run(api_request_json)\n",
    "print(json.dumps(response.json(), indent=2))\n"
   ],
   "id": "a3a77ea57786ac66",
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "POST 401 Insufficient balance.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 39\u001B[0m\n\u001B[0;32m      8\u001B[0m api_request_json \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllama3.1-70b\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunction_call\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mget_current_weather\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     36\u001B[0m }\n\u001B[0;32m     38\u001B[0m \u001B[38;5;66;03m# Execute the Request\u001B[39;00m\n\u001B[1;32m---> 39\u001B[0m response \u001B[38;5;241m=\u001B[39m llama\u001B[38;5;241m.\u001B[39mrun(api_request_json)\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28mprint\u001B[39m(json\u001B[38;5;241m.\u001B[39mdumps(response\u001B[38;5;241m.\u001B[39mjson(), indent\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m))\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\llamaapi\\llamaapi.py:67\u001B[0m, in \u001B[0;36mLlamaAPI.run\u001B[1;34m(self, api_request_json)\u001B[0m\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_stream(api_request_json)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 67\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_sync(api_request_json)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\llamaapi\\llamaapi.py:53\u001B[0m, in \u001B[0;36mLlamaAPI.run_sync\u001B[1;34m(self, api_request_json)\u001B[0m\n\u001B[0;32m     51\u001B[0m response \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mpost(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhostname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdomain_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, headers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mheaders, json\u001B[38;5;241m=\u001B[39mapi_request_json)\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m200\u001B[39m:        \n\u001B[1;32m---> 53\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPOST \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mjson()[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdetail\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[1;31mException\u001B[0m: POST 401 Insufficient balance."
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T13:51:29.921720Z",
     "start_time": "2024-10-29T13:51:25.879670Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install openai",
   "id": "ebf5d5012c7a40ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\anwender\\anaconda3\\lib\\site-packages (1.52.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from openai) (1.10.18)\n",
      "Requirement already satisfied: sniffio in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\anwender\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T13:52:03.234597Z",
     "start_time": "2024-10-29T13:52:02.925346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#https://platform.openai.com/docs/quickstart?desktop-os=windows&language-preference=python\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a haiku about recursion in programming.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n",
    "\n"
   ],
   "id": "94ee56c3b120250c",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'model_json_schema' from 'openai._compat' (C:\\Users\\Anwender\\anaconda3\\Lib\\site-packages\\openai\\_compat.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mopenai\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OpenAI\n\u001B[0;32m      2\u001B[0m client \u001B[38;5;241m=\u001B[39m OpenAI()\n\u001B[0;32m      4\u001B[0m completion \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mchat\u001B[38;5;241m.\u001B[39mcompletions\u001B[38;5;241m.\u001B[39mcreate(\n\u001B[0;32m      5\u001B[0m     model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt-4o-mini\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      6\u001B[0m     messages\u001B[38;5;241m=\u001B[39m[\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m     ]\n\u001B[0;32m     13\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\__init__.py:11\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_types\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m NOT_GIVEN, NoneType, NotGiven, Transport, ProxiesTypes\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m file_from_path\n\u001B[1;32m---> 11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_client\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Client, OpenAI, Stream, Timeout, Transport, AsyncClient, AsyncOpenAI, AsyncStream, RequestOptions\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_models\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseModel\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_version\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __title__, __version__\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_client.py:11\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping_extensions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Self, override\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mhttpx\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m resources, _exceptions\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_qs\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Querystring\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_types\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     14\u001B[0m     NOT_GIVEN,\n\u001B[0;32m     15\u001B[0m     Omit,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     20\u001B[0m     RequestOptions,\n\u001B[0;32m     21\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\resources\\__init__.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbeta\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      4\u001B[0m     Beta,\n\u001B[0;32m      5\u001B[0m     AsyncBeta,\n\u001B[0;32m      6\u001B[0m     BetaWithRawResponse,\n\u001B[0;32m      7\u001B[0m     AsyncBetaWithRawResponse,\n\u001B[0;32m      8\u001B[0m     BetaWithStreamingResponse,\n\u001B[0;32m      9\u001B[0m     AsyncBetaWithStreamingResponse,\n\u001B[0;32m     10\u001B[0m )\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     12\u001B[0m     Chat,\n\u001B[0;32m     13\u001B[0m     AsyncChat,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     17\u001B[0m     AsyncChatWithStreamingResponse,\n\u001B[0;32m     18\u001B[0m )\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01maudio\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     20\u001B[0m     Audio,\n\u001B[0;32m     21\u001B[0m     AsyncAudio,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     25\u001B[0m     AsyncAudioWithStreamingResponse,\n\u001B[0;32m     26\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\resources\\beta\\__init__.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbeta\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      4\u001B[0m     Beta,\n\u001B[0;32m      5\u001B[0m     AsyncBeta,\n\u001B[0;32m      6\u001B[0m     BetaWithRawResponse,\n\u001B[0;32m      7\u001B[0m     AsyncBetaWithRawResponse,\n\u001B[0;32m      8\u001B[0m     BetaWithStreamingResponse,\n\u001B[0;32m      9\u001B[0m     AsyncBetaWithStreamingResponse,\n\u001B[0;32m     10\u001B[0m )\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mthreads\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     12\u001B[0m     Threads,\n\u001B[0;32m     13\u001B[0m     AsyncThreads,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     17\u001B[0m     AsyncThreadsWithStreamingResponse,\n\u001B[0;32m     18\u001B[0m )\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01massistants\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     20\u001B[0m     Assistants,\n\u001B[0;32m     21\u001B[0m     AsyncAssistants,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     25\u001B[0m     AsyncAssistantsWithStreamingResponse,\n\u001B[0;32m     26\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\resources\\beta\\beta.py:5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m annotations\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mthreads\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      6\u001B[0m     Threads,\n\u001B[0;32m      7\u001B[0m     AsyncThreads,\n\u001B[0;32m      8\u001B[0m     ThreadsWithRawResponse,\n\u001B[0;32m      9\u001B[0m     AsyncThreadsWithRawResponse,\n\u001B[0;32m     10\u001B[0m     ThreadsWithStreamingResponse,\n\u001B[0;32m     11\u001B[0m     AsyncThreadsWithStreamingResponse,\n\u001B[0;32m     12\u001B[0m )\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_compat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cached_property\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01massistants\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     15\u001B[0m     Assistants,\n\u001B[0;32m     16\u001B[0m     AsyncAssistants,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     20\u001B[0m     AsyncAssistantsWithStreamingResponse,\n\u001B[0;32m     21\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\resources\\beta\\threads\\__init__.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mruns\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      4\u001B[0m     Runs,\n\u001B[0;32m      5\u001B[0m     AsyncRuns,\n\u001B[0;32m      6\u001B[0m     RunsWithRawResponse,\n\u001B[0;32m      7\u001B[0m     AsyncRunsWithRawResponse,\n\u001B[0;32m      8\u001B[0m     RunsWithStreamingResponse,\n\u001B[0;32m      9\u001B[0m     AsyncRunsWithStreamingResponse,\n\u001B[0;32m     10\u001B[0m )\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mthreads\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     12\u001B[0m     Threads,\n\u001B[0;32m     13\u001B[0m     AsyncThreads,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     17\u001B[0m     AsyncThreadsWithStreamingResponse,\n\u001B[0;32m     18\u001B[0m )\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmessages\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     20\u001B[0m     Messages,\n\u001B[0;32m     21\u001B[0m     AsyncMessages,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     25\u001B[0m     AsyncMessagesWithStreamingResponse,\n\u001B[0;32m     26\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\resources\\beta\\threads\\runs\\__init__.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mruns\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      4\u001B[0m     Runs,\n\u001B[0;32m      5\u001B[0m     AsyncRuns,\n\u001B[0;32m      6\u001B[0m     RunsWithRawResponse,\n\u001B[0;32m      7\u001B[0m     AsyncRunsWithRawResponse,\n\u001B[0;32m      8\u001B[0m     RunsWithStreamingResponse,\n\u001B[0;32m      9\u001B[0m     AsyncRunsWithStreamingResponse,\n\u001B[0;32m     10\u001B[0m )\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msteps\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     12\u001B[0m     Steps,\n\u001B[0;32m     13\u001B[0m     AsyncSteps,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     17\u001B[0m     AsyncStepsWithStreamingResponse,\n\u001B[0;32m     18\u001B[0m )\n\u001B[0;32m     20\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSteps\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsyncSteps\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsyncRunsWithStreamingResponse\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     33\u001B[0m ]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\resources\\beta\\threads\\runs\\runs.py:34\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_streaming\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Stream, AsyncStream\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpagination\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SyncCursorPage, AsyncCursorPage\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_base_client\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m---> 34\u001B[0m     AsyncPaginator,\n\u001B[0;32m     35\u001B[0m     make_request_options,\n\u001B[0;32m     36\u001B[0m )\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstreaming\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     38\u001B[0m     AssistantEventHandler,\n\u001B[0;32m     39\u001B[0m     AssistantEventHandlerT,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     43\u001B[0m     AsyncAssistantStreamManager,\n\u001B[0;32m     44\u001B[0m )\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtypes\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbeta\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mthreads\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     46\u001B[0m     run_list_params,\n\u001B[0;32m     47\u001B[0m     run_create_params,\n\u001B[0;32m     48\u001B[0m     run_update_params,\n\u001B[0;32m     49\u001B[0m     run_submit_tool_outputs_params,\n\u001B[0;32m     50\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\lib\\__init__.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_tools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pydantic_function_tool \u001B[38;5;28;01mas\u001B[39;00m pydantic_function_tool\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_parsing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ResponseFormatT \u001B[38;5;28;01mas\u001B[39;00m ResponseFormatT\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\lib\\_tools.py:7\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Any, Dict, cast\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpydantic\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_pydantic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m to_strict_json_schema\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtypes\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ChatCompletionToolParam\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtypes\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mshared_params\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FunctionDefinition\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\lib\\_pydantic.py:11\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_types\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m NOT_GIVEN\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_dict \u001B[38;5;28;01mas\u001B[39;00m _is_dict, is_list\n\u001B[1;32m---> 11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_compat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PYDANTIC_V2, model_json_schema\n\u001B[0;32m     13\u001B[0m _T \u001B[38;5;241m=\u001B[39m TypeVar(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_T\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mto_strict_json_schema\u001B[39m(model: \u001B[38;5;28mtype\u001B[39m[pydantic\u001B[38;5;241m.\u001B[39mBaseModel] \u001B[38;5;241m|\u001B[39m pydantic\u001B[38;5;241m.\u001B[39mTypeAdapter[Any]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]:\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'model_json_schema' from 'openai._compat' (C:\\Users\\Anwender\\anaconda3\\Lib\\site-packages\\openai\\_compat.py)"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:12:59.016414Z",
     "start_time": "2024-10-29T16:12:58.324577Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install ollama",
   "id": "7ca5a2b8c7301944",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\r\n",
      "  Downloading ollama-0.3.3-py3-none-any.whl.metadata (3.8 kB)\r\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /opt/anaconda3/envs/DataAndKnowledgeEngineering/lib/python3.8/site-packages (from ollama) (0.27.0)\r\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/DataAndKnowledgeEngineering/lib/python3.8/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.2.0)\r\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/DataAndKnowledgeEngineering/lib/python3.8/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.8.30)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/DataAndKnowledgeEngineering/lib/python3.8/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.2)\r\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/DataAndKnowledgeEngineering/lib/python3.8/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.7)\r\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/DataAndKnowledgeEngineering/lib/python3.8/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.0)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/DataAndKnowledgeEngineering/lib/python3.8/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/DataAndKnowledgeEngineering/lib/python3.8/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (1.2.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /opt/anaconda3/envs/DataAndKnowledgeEngineering/lib/python3.8/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (4.11.0)\r\n",
      "Downloading ollama-0.3.3-py3-none-any.whl (10 kB)\r\n",
      "Installing collected packages: ollama\r\n",
      "Successfully installed ollama-0.3.3\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:26:51.788326Z",
     "start_time": "2024-10-29T16:26:48.009294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=\"llama3.2\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me an interesting fact about elephants\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(response[\"message\"][\"content\"])"
   ],
   "id": "64de44d737780c13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a fascinating fact about elephants:\n",
      "\n",
      "Elephants have a highly developed sense of empathy and can recognize and respond to the emotions of others. In fact, they are considered one of the most empathetic animals on Earth.\n",
      "\n",
      "Studies have shown that when an elephant experiences stress or distress, its family members will often gather around it and provide support and comfort. They may even touch each other with their trunks in a gentle, reassuring manner.\n",
      "\n",
      "This empathy is thought to be linked to the large size and complexity of an elephant's brain, which contains a high concentration of neurons that are dedicated to emotional processing. It's believed that this advanced emotional intelligence allows elephants to pick up on subtle social cues and respond with compassion and care.\n",
      "\n",
      "Isn't that amazing?\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T16:13:24.073763Z",
     "start_time": "2024-11-04T16:13:22.051020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ollama\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Function to read the content of a PDF document\n",
    "def read_pdf(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Replace this with the path to your PDF document\n",
    "file_path = \"resources/business_rules_document.pdf\"\n",
    "\n",
    "# Read the content of the PDF document\n",
    "document_content = read_pdf(file_path)\n",
    "\n",
    "# Create the Ollama API request\n",
    "response = ollama.chat(\n",
    "    model=\"llama3.2\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"I have the following document: {document_content}\\n\\n extract all the business rules from this document. Present them clearly in a bullet-point format.\"\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Extract and print the response\n",
    "print(response[\"message\"][\"content\"])\n"
   ],
   "id": "670c08e96921729c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the extracted business rules in bullet-point format:\n",
      "\n",
      "• **Customer Opening an Account**: \n",
      "  • Customers must provide valid identification before opening an account.\n",
      "\n",
      "• **Transaction Reporting**:\n",
      "  • All transactions above $10,000 must be reported to the compliance department.\n",
      "\n",
      "• **Loan Application Approval**:\n",
      "  • Loan applications can only be processed if the applicant has a credit score of 650 or above.\n",
      "\n",
      "• **Expense Report Approval**:\n",
      "  • Employees are not allowed to approve their own expense reports.\n",
      "\n",
      "• **Refund Policy**:\n",
      "  • Refunds are only issued if the request is made within 30 days of the purchase date.\n",
      "\n",
      "• **Discount Approval**:\n",
      "  • A manager must approve any discount greater than 20%.\n",
      "\n",
      "• **Terms and Conditions Agreement**:\n",
      "  • Customers must agree to the terms and conditions before using the service.\n",
      "\n",
      "• **Customer Data Security**:\n",
      "  • All customer data must be encrypted when stored or transmitted.\n",
      "\n",
      "• **Vendor Approval Process**:\n",
      "  • Vendors must be vetted and approved by the procurement team before any contracts are signed.\n",
      "\n",
      "• **Inventory Management**:\n",
      "  • Inventory checks must be conducted at the end of each month.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T16:31:30.792413Z",
     "start_time": "2024-11-04T16:30:36.983558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ollama\n",
    "\n",
    "# Initialize the Ollama model\n",
    "#ollama.initialize_model(\"ollama-legal-extraction\")\n",
    "\n",
    "def extract_rules_from_text(text):\n",
    "    # Step 1: Segment and preprocess the text\n",
    "    #segments = ollama.segment_text(text)\n",
    "    segmentsResponse = ollama.chat(\n",
    "        model=\"llama3.2\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"segment the text into logical segments use # to separate the segments: {text}\"\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    segments = segmentsResponse[\"message\"][\"content\"].split(\"#\")\n",
    "\n",
    "    extracted_rules = []\n",
    "\n",
    "    for segment in segments:\n",
    "        # Step 2: Extract rules using contextual understanding\n",
    "        prompt = f\"Identify legal rules in the following segment: {segment}\"\n",
    "        #response = ollama.generate(prompt)\n",
    "        response = ollama.chat(\n",
    "            model=\"llama3.2\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Step 3: Parse the response to get conditions and consequences\n",
    "        #parsed_rules = parse_response(response)\n",
    "        extracted_rules.append(response[\"message\"][\"content\"])\n",
    "\n",
    "    return extracted_rules\n",
    "\n",
    "def convert_to_prolog(logical_rules):\n",
    "    prolog_rules = []\n",
    "\n",
    "    for rule in logical_rules:\n",
    "        # Convert logical rule to Prolog format\n",
    "        prompt = f\"Convert this rule to Prolog: {rule}\"\n",
    "        responseProlog = ollama.chat(\n",
    "            model=\"llama3.2\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "        prolog_rules.append(responseProlog[\"message\"][\"content\"])\n",
    "\n",
    "    return prolog_rules\n",
    "\n",
    "# Example usage\n",
    "# Replace this with the path to your PDF document\n",
    "file_path = \"resources/business_rules_document.pdf\"\n",
    "\n",
    "# Read the content of the PDF document\n",
    "legal_text = read_pdf(file_path)\n",
    "rules = extract_rules_from_text(legal_text)\n",
    "prolog_rules = convert_to_prolog(rules)\n",
    "prolog_rules_string = \"\\n\".join(prolog_rules)\n",
    "\n",
    "output = ollama.chat(\n",
    "    model=\"llama3.2\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Just return prolog rules in this text, don't simplify: {prolog_rules_string}\"\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Output the Prolog rules\n",
    "print(output[\"message\"][\"content\"])"
   ],
   "id": "3153f6e4491d43fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an updated version of your code in Python:\n",
      "\n",
      "```python\n",
      "class DueDiligenceRule:\n",
      "    def __init__(self):\n",
      "        # Inability to meet requirements as a reason for partnering with an unqualified vendor\n",
      "        self.inability_to_meet_requirements = [\n",
      "            \"inability_to_deliver_within_budget\",\n",
      "            \"inability_to_meet_quality_standards\"\n",
      "        ]\n",
      "\n",
      "    def get_due_diligence_procedures(self):\n",
      "        return [\"require_vetting_and_approval\"]\n",
      "\n",
      "class VenderSuitabilityRule:\n",
      "    def __init__(self, suitability_criteria):\n",
      "        # Suitability criteria for vendors\n",
      "        self.suitability_criteria = suitability_criteria\n",
      "\n",
      "    def get_requirements_for_good_time_management(self):\n",
      "        return \"require_good_time_management\"\n",
      "\n",
      "    def get_cost_management_requirements(self):\n",
      "        return \"cost_management_requirements\"\n",
      "\n",
      "    def get_quality_standards_requirements(self):\n",
      "        return \"require_quality_standards\"\n",
      "\n",
      "\n",
      "class ContractualClause:\n",
      "    def __init__(self, contractual_clause_requirement):\n",
      "        # Contractual clause requirement for due diligence\n",
      "        self.contractual_clause_requirement = contractual_clause_requirement\n",
      "\n",
      "    def get_due_diligence_procedures(self):\n",
      "        return [\"due_diligence_procedures\"]\n",
      "\n",
      "    def get_approval_processes(self):\n",
      "        return \"approval_processes\"\n",
      "\n",
      "\n",
      "class PolicyOrProcedure:\n",
      "    def __init__(self, policy_or_procedure_requirement):\n",
      "        # Policy or procedure requirement for vendor vetting and approval\n",
      "        self.policy_or_procedure_requirement = policy_or_procedure_requirement\n",
      "\n",
      "    def get_vetting_and_approving_requirements(self):\n",
      "        return \"vetting_and_approving_requirements\"\n",
      "\n",
      "    def get_procurement_team_evaluation(self):\n",
      "        return \"procurement_team_evaluation\"\n",
      "\n",
      "\n",
      "class RelevantLawsAndRegulations:\n",
      "    def __init__(self, relevant_law_or_regulation):\n",
      "        # Relevant laws and regulations governing public procurement\n",
      "        self.relevant_law_or_regulation = relevant_law_or_regulation\n",
      "\n",
      "    def get_far_federal_acquisition_rationale(self):\n",
      "        return \"far_federal_acquisition_rationale\"\n",
      "\n",
      "    def get_requirement_for_thorough_reviews(self):\n",
      "        return \"requirement_for_thorough_reviews\"\n",
      "\n",
      "\n",
      "# Example usage:\n",
      "rule = DueDiligenceRule()\n",
      "print(rule.get_due_diligence_procedures())  # Output: ['require_vetting_and_approval']\n",
      "\n",
      "suitability_rule = VenderSuitabilityRule([{\"require_good_time_management\": True},\n",
      "                                         {\"cost_management_requirements\": True},\n",
      "                                         {\"require_quality_standards\": True}])\n",
      "print(suitability_rule.get_requirements_for_good_time_management())  # Output: require_good_time_management\n",
      "print(suitability_rule.get_cost_management_requirements())  # Output: cost_management_requirements\n",
      "print(suitability_rule.get_quality_standards_requirements())  # Output: require_quality_standards\n",
      "\n",
      "contractual_clause = ContractualClause(\"contractual_clause_requirement\")\n",
      "print(contractual_clause.get_due_diligence_procedures())  # Output: ['due_diligence_procedures']\n",
      "print(contractual_clause.get_approval_processes())  # Output: approval_processes\n",
      "\n",
      "policy_or_procedure = PolicyOrProcedure(\"policy_or_procedure_requirement\")\n",
      "print(policy_or_procedure.get_vetting_and_approving_requirements())  # Output: vetting_and_approving_requirements\n",
      "print(policy_or_procedure.get_procurement_team_evaluation())  # Output: procurement_team_evaluation\n",
      "\n",
      "relevant_laws_and_regulations = RelevantLawsAndRegulations(\"relevant_law_or_regulation\")\n",
      "print(relevant_laws_and_regulations.get_far_federal_acquisition_rationale())  # Output: far_federal_acquisition_rationale\n",
      "print(relevant_laws_and_regulations.get_requirement_for_thorough_reviews())  # Output: requirement_for_thorough_reviews\n",
      "```\n",
      "\n",
      "This code defines classes for the different components of the due diligence rule, including `DueDiligenceRule`, `VenderSuitabilityRule`, `ContractualClause`, `PolicyOrProcedure`, and `RelevantLawsAndRegulations`. Each class has methods that return relevant data or procedures. The example usage demonstrates how to create instances of these classes and call their methods.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b59de4d9edb1f9f1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
